{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\learningsoft\\anadonda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#======================加载word2vec模型==========================\n",
    "from gensim.models import Word2Vec\n",
    "wv = Word2Vec.load(\"F:/Jupyter/--NLP/big_things/models/wikibaikeWV250/wikibaikewv250\")\n",
    "vocab = wv.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##=====================加载LTP模型 ========================\n",
    "import os\n",
    "LTP_DATA_DIR = 'F:/MyDownloads/ltp_data_v3.4.0/ltp_data_v3.4.0'  # ltp模型目录的路径\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径\n",
    "from pyltp import Segmentor # 分词器\n",
    "from pyltp import Postagger # 词性标注器\n",
    "from pyltp import Parser # 句法分析器\n",
    "\n",
    "segmentor = Segmentor()  # 初始化实例\n",
    "# segmentor.load(cws_model_path)  \n",
    "segmentor.load_with_lexicon(cws_model_path, 'userword.txt')# 加载模型\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`\n",
    "postagger = Postagger() # 初始化实例\n",
    "postagger.load(pos_model_path)  # 加载模型\n",
    "par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  # 依存句法分析模型路径，模型名称为`parser.model`\n",
    "parser = Parser() # 初始化实例\n",
    "parser.load(par_model_path)  # 加载模型\n",
    "\n",
    "relation_dic = {'SBV':'主谓关系','VOB':'动宾关系','IOB':'间宾关系','FOB':'前置宾语',\\\n",
    "                'DBL':'兼语','ATT':'定中关系','ADV':'状中结构','CMP':'动补结构',\\\n",
    "                'COO':'并列关系','POB':'介宾关系','LAD':'左附加关系','RAD':'右附加关系','IS':'独立结构','HED':'核心关系','WP':'标点'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27191\n",
      "133\n"
     ]
    }
   ],
   "source": [
    "# ==================加载百科词频文件===================\n",
    "import json\n",
    "wiki_word_freq_json = open('F:/Jupyter/--NLP/big_things/wiki_word_freq.json','r',encoding='utf-8').read()\n",
    "wiki_word_freq_dic = json.loads(wiki_word_freq_json)\n",
    "print(wiki_word_freq_dic['能力'])\n",
    "print(wiki_word_freq_dic['java'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\learningsoft\\anadonda\\lib\\site-packages\\ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "#==========================Utility funcitons===========================\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_words_and_tags(sentence):\n",
    "    words = list(segmentor.segment(sentence))\n",
    "    postags = list(postagger.postag(words))\n",
    "    return [words,postags]\n",
    "\n",
    "\"\"\"\n",
    "计算一组词的综合词向量，有若干方法：\n",
    "1.简单地直接计算平均，没有的词直接忽略\n",
    "2.同上，但是没有的词，拆成单字来计算词向量\n",
    "3.在上一步的基础上，加上百科词频来赋予权重，然后加权平均\n",
    "4.再加上位置权重\n",
    "\"\"\"\n",
    "# 根据百科词频计算词权重：\n",
    "import math\n",
    "def wiki_weight(word):\n",
    "    try:\n",
    "        freq = wiki_word_freq_dic[word]\n",
    "    except KeyError:\n",
    "        freq = 1\n",
    "    return 1/(math.log10(freq+1))\n",
    "\n",
    "shape_vec = wv[\"加油\"]\n",
    "unk_wv = {}\n",
    "\n",
    "# 计算位置权重：\n",
    "def loc_weight(w_list):\n",
    "    list_len=len(w_list)\n",
    "    i =1-(list_len-1)*0.1\n",
    "    loc_weight_list=[]\n",
    "    for wd in w_list:\n",
    "        loc_weight_list.append(i)\n",
    "        i =i+0.05\n",
    "    return (loc_weight_list)\n",
    "    \n",
    "class Phrase_vec:\n",
    "    # 1.简单地直接计算平均，没有的词直接忽略\n",
    "    def simpleAvgVec(self,words):\n",
    "        vec = np.zeros_like(shape_vec)\n",
    "        n = 0\n",
    "        for word in words:\n",
    "            if word in vocab.keys():\n",
    "                vec += wv[word]\n",
    "                n += 1\n",
    "        if n>0:\n",
    "            return vec/n\n",
    "        else:\n",
    "            return vec\n",
    "    # 2.同上，但是没有的词，拆成单字来计算词向量\n",
    "    def avgVec(self,words):\n",
    "        vec = np.zeros_like(shape_vec)\n",
    "        n = 0\n",
    "        for word in words:\n",
    "            if word in vocab.keys():\n",
    "                vec += wv[word]\n",
    "                n += 1\n",
    "            else:\n",
    "#                 print(word,\" NOT IN WV-DICT ! USE CHAR-VEC\")\n",
    "                chars = [c for c in word]\n",
    "                unk_vec = Phrase_vec.simpleAvgVec(self,chars)*len(word)\n",
    "                vec += unk_vec\n",
    "                n += 1\n",
    "                unk_wv[word] = unk_vec\n",
    "        if n>0:\n",
    "            return vec/n\n",
    "        else:\n",
    "            return vec\n",
    "    \n",
    "    # 3.在上一步的基础上，加上百科词频来赋予权重，然后加权平均\n",
    "    def weightedAvgVec1(self,words):\n",
    "        vec = np.zeros_like(shape_vec)\n",
    "        n = 0\n",
    "        for word in words:\n",
    "            weight = wiki_weight(word)\n",
    "            if word in vocab.keys():\n",
    "                vec += wv[word]*weight\n",
    "                n += 1\n",
    "            else:\n",
    "#                 print(word,\" NOT IN WV-DICT ! USE CHAR-VEC\")\n",
    "                chars = [c for c in word]\n",
    "                unk_vec = Phrase_vec.simpleAvgVec(self,chars)*len(word)\n",
    "                vec += unk_vec*weight\n",
    "                n += 1\n",
    "                unk_wv[word] = unk_vec\n",
    "        if n>0:\n",
    "            return vec/n\n",
    "        else:\n",
    "            return vec\n",
    "    \n",
    "    # 4.再加上位置权重\n",
    "    def weightedAvgVec2(self,words):\n",
    "        loc_weights = loc_weight(words)\n",
    "        vec = np.zeros_like(shape_vec)\n",
    "        n = 0\n",
    "        for word,l_w in zip(words,loc_weights):\n",
    "            weight = wiki_weight(word)\n",
    "            if word in vocab.keys():\n",
    "                vec += wv[word]*weight*l_w\n",
    "                n += 1\n",
    "            else:\n",
    "#                 print(word,\" NOT IN WV-DICT ! USE CHAR-VEC\")\n",
    "                chars = [c for c in word]\n",
    "                unk_vec = Phrase_vec.simpleAvgVec(self,chars)*len(word)\n",
    "                vec += unk_vec*weight*l_w\n",
    "                n += 1\n",
    "                unk_wv[word] = unk_vec\n",
    "        if n>0:\n",
    "            return vec/n\n",
    "        else:\n",
    "            return vec\n",
    "    \n",
    "    \"\"\"\n",
    "    一些短语向量的衡量指标：\n",
    "    ATM: 每个词和整个短语平均词向量的余弦距离\n",
    "    Conicity：锥度，所有ATM的平均值，越大，说明各个词越相近/越聚集\n",
    "    VS（vector spread）：向量散度，衡量各个词的分散性\n",
    "    \"\"\"\n",
    "    def atm(self,word,words):\n",
    "        words_avg_vec = Phrase_vec.avgVec(self,words).reshape(1,250)\n",
    "        current_vec = Phrase_vec.avgVec(self,[word]).reshape(1,250)\n",
    "        atm_score = cosine_similarity(current_vec,words_avg_vec)[0][0]\n",
    "        return atm_score\n",
    "    def conicity_vs(self,words):\n",
    "        atms = [Phrase_vec.atm(self,word,words) for word in words]\n",
    "        sum = 0\n",
    "        for atm in atms:\n",
    "            sum += atm\n",
    "        conicity_score = sum/len(words)\n",
    "        minus_square = 0\n",
    "        for atm in atms:\n",
    "            minus_square += pow((atm-conicity_score),2)\n",
    "        vs_score = minus_square/len(words)\n",
    "        return [10*conicity_score,1000*vs_score]\n",
    "        \n",
    "\n",
    "p_vec = Phrase_vec()\n",
    "\n",
    "def getSimiScore(words,target_words):\n",
    "    words_vec = p_vec.weightedAvgVec1(words).reshape(1,250)\n",
    "    target_vec = p_vec.weightedAvgVec1(target_words).reshape(1,250)\n",
    "    return cosine_similarity(words_vec,target_vec)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.799393385648727   7.039833068847656   6.8465059995651245\n",
      "0.05617520905221163   0.5096023786546056   7.38286402438959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\learningsoft\\anadonda\\lib\\site-packages\\ipykernel_launcher.py:58: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "conicity,vs两个指标的试验：\n",
    "\"\"\"\n",
    "conicity,vs = p_vec.conicity_vs(['开心','高兴','兴奋','心情'])\n",
    "conicity1,vs1 = p_vec.conicity_vs(['信息','管理','系统','开发'])\n",
    "conicity2,vs2 = p_vec.conicity_vs(['昨天','电脑','物理','书籍'])\n",
    "print(conicity,' ',conicity1,' ',conicity2)\n",
    "print(vs,' ',vs1,' ',vs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#=======================Sum up, get final results==================\n",
    "\"\"\"\n",
    "input: \n",
    "    sentence/短语\n",
    "output:\n",
    "    word_pairs---list\n",
    "    relations---list\n",
    "    simiscores---list\n",
    "\"\"\"\n",
    "def getParserResult(sentence):\n",
    "    words,postags = get_words_and_tags(sentence)\n",
    "    conicity,vs = p_vec.conicity_vs(words)\n",
    "    arcs = parser.parse(words, postags)  # 句法分析\n",
    "\n",
    "    results = ''\n",
    "    \n",
    "    word_pairs = []\n",
    "    relations = []\n",
    "    simiscores = []\n",
    "\n",
    "    for index,arc in enumerate(arcs):\n",
    "        word = words[index]\n",
    "        relation_word = words[arc.head-1] if arc.head>0 else 'ROOT'\n",
    "        relation = arc.relation\n",
    "        combination = words[index]+'-'+relation_word if index<=arc.head-1 else relation_word+'-'+words[index]\n",
    "        tag = postags[index]\n",
    "        relation_tag = postags[arc.head-1] if arc.head>0 else 'ROOT'\n",
    "        \n",
    "        # 要输出的关系：\n",
    "        if relation not in ['HED','LAD','RAD','POB','IS','WP','CMP'] :\n",
    "            # 获取word_pair，并计算相似度。然后添加到相关的list中\n",
    "            word_pair = set([word,relation_word])\n",
    "            simiscore = getSimiScore(words,list(word_pair))\n",
    "            word_pairs.append(word_pair)\n",
    "            relations.append(relation)\n",
    "            simiscores.append(simiscore)\n",
    "            \n",
    "            if relation == 'COO':\n",
    "                # 必须新建一个list，再用新list来循环，否则会进入死循环！\n",
    "                ori_word_pairs = word_pairs[:] \n",
    "                for the_pair in ori_word_pairs:\n",
    "                    cross = word_pair&the_pair\n",
    "                    if len(cross) == 1 and 'ROOT' not in the_pair: # 发现重合,但不能是其COO-pair本身\n",
    "                        whole = word_pair|the_pair\n",
    "                        new_pair = whole - cross\n",
    "                        simiscore = getSimiScore(words,list(new_pair))\n",
    "                        word_pairs.append(new_pair)\n",
    "                        relations.append(\"New!\")\n",
    "                        simiscores.append(simiscore)\n",
    "                        \n",
    "    return [word_pairs,relations,simiscores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "ann_filenames = []\n",
    "phrases = []\n",
    "kind = 'TEST'\n",
    "base_path = 'F:/Jupyter/--NLP/big_things/resume_txt_ann_6kinds'\n",
    "files = os.listdir(base_path+\"/\"+kind)\n",
    "ann_filenames = [f for f in files if os.path.splitext(f)[-1]=='.ann']\n",
    "for index,ann_filename in enumerate(ann_filenames):\n",
    "    with open(base_path+\"/\"+kind+\"/\"+ann_filename,encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            phrase = l.split('\\t')[-1].strip()\n",
    "#             phrase = re.sub('[a-zA-Z]','',phrase)\n",
    "            if len(jieba.lcut(l))>=2:\n",
    "#             if len(phrase)>4:\n",
    "                phrases.append(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标注短语：\n",
      "【验证线上版本】\n",
      "{'验证', '线'}  ||  ATT  ||  0.88802004\n",
      "{'上', '版本'}  ||  ATT  ||  0.7902255\n",
      "{'上', '线'}  ||  ATT  ||  0.7553108\n",
      "========================\n",
      "标注短语：\n",
      "【测试仪器的维护和点检】\n",
      "{'仪器', '点检'}  ||  New!  ||  0.85611224\n",
      "{'维护', '点检'}  ||  COO  ||  0.83868194\n",
      "{'仪器', '维护'}  ||  ATT  ||  0.8062717\n",
      "{'仪器', '测试'}  ||  ATT  ||  0.7709675\n",
      "========================\n",
      "标注短语：\n",
      "【产品的标识】\n",
      "{'标识', '产品'}  ||  ATT  ||  0.97434413\n",
      "========================\n",
      "标注短语：\n",
      "【有效周期的维护和确认】\n",
      "{'维护', '周期'}  ||  ATT  ||  0.8513347\n",
      "{'确认', '周期'}  ||  New!  ||  0.8353963\n",
      "{'有效', '周期'}  ||  ATT  ||  0.833671\n",
      "{'维护', '确认'}  ||  COO  ||  0.8236501\n",
      "========================\n",
      "标注短语：\n",
      "【IQC日报表】\n",
      "{'IQC', '日报表'}  ||  ATT  ||  1.0\n",
      "========================\n",
      "标注短语：\n",
      "【烟尘处理器】\n",
      "{'处理器', '烟尘'}  ||  ATT  ||  1.0000001\n",
      "========================\n",
      "标注短语：\n",
      "【采集样品数据】\n",
      "{'采集', '样品'}  ||  ATT  ||  0.95344424\n",
      "{'数据', '样品'}  ||  ATT  ||  0.9237138\n",
      "========================\n",
      "标注短语：\n",
      "【FPC组装】\n",
      "{'FPC', '组装'}  ||  SBV  ||  1.0\n",
      "========================\n",
      "标注短语：\n",
      "【FR4冲压】\n",
      "{'冲压', 'FR4'}  ||  ATT  ||  1.0000001\n",
      "========================\n",
      "标注短语：\n",
      "【ISO14000质量】\n",
      "{'质量', 'ISO14000'}  ||  ATT  ||  0.9999998\n",
      "========================\n",
      "标注短语：\n",
      "【团队建设和发展】\n",
      "{'团队', '建设'}  ||  FOB  ||  0.9133179\n",
      "{'团队', '发展'}  ||  New!  ||  0.8962901\n",
      "{'发展', '建设'}  ||  COO  ||  0.8750663\n",
      "========================\n",
      "标注短语：\n",
      "【客诉原因分析】\n",
      "{'客诉', '原因'}  ||  ATT  ||  0.9509655\n",
      "{'原因', '分析'}  ||  FOB  ||  0.87151456\n",
      "========================\n",
      "标注短语：\n",
      "【绩效考核与评定】\n",
      "{'评定', '绩效考核'}  ||  COO  ||  0.9770695\n",
      "========================\n",
      "标注短语：\n",
      "【加工与装配服务】\n",
      "{'装配', '加工'}  ||  COO  ||  0.92310226\n",
      "{'服务', '装配'}  ||  New!  ||  0.89928037\n",
      "{'服务', '加工'}  ||  COO  ||  0.8708874\n",
      "========================\n",
      "标注短语：\n",
      "【plc线路集成】\n",
      "{'线路', 'plc'}  ||  ATT  ||  0.9035049\n",
      "{'线路', '集成'}  ||  SBV  ||  0.82404345\n",
      "========================\n",
      "标注短语：\n",
      "【核电操作台】\n",
      "{'核电', '操作台'}  ||  ATT  ||  0.99999994\n",
      "========================\n",
      "标注短语：\n",
      "【质量检测部门】\n",
      "{'质量', '检测'}  ||  FOB  ||  0.9277385\n",
      "{'部门', '检测'}  ||  ATT  ||  0.91047686\n",
      "========================\n",
      "标注短语：\n",
      "【电气成套设备】\n",
      "{'设备', '成套'}  ||  ATT  ||  0.9210912\n",
      "{'设备', '电气'}  ||  ATT  ||  0.90196955\n",
      "========================\n",
      "标注短语：\n",
      "【高效等仪器】\n",
      "{'仪器', '高效'}  ||  ATT  ||  0.9597534\n",
      "========================\n",
      "标注短语：\n",
      "【中药提取与检验】\n",
      "{'提取', '中药'}  ||  SBV  ||  0.9185109\n",
      "{'检验', '中药'}  ||  New!  ||  0.914343\n",
      "{'提取', '检验'}  ||  COO  ||  0.882355\n",
      "========================\n",
      "标注短语：\n",
      "【品质问题检讨】\n",
      "{'检讨', '问题'}  ||  FOB  ||  0.910485\n",
      "{'问题', '品质'}  ||  ATT  ||  0.8687645\n",
      "========================\n",
      "标注短语：\n",
      "【新项目研发打样】\n",
      "{'研发', '打样'}  ||  VOB  ||  0.9179944\n",
      "{'研发', '项目'}  ||  FOB  ||  0.85776037\n",
      "{'新', '项目'}  ||  ATT  ||  0.7889118\n",
      "========================\n",
      "标注短语：\n",
      "【售后不良分析与检讨】\n",
      "{'检讨', '不良'}  ||  New!  ||  0.81970274\n",
      "{'后', '检讨'}  ||  New!  ||  0.77202356\n",
      "{'分析', '检讨'}  ||  COO  ||  0.7639643\n",
      "{'分析', '不良'}  ||  ATT  ||  0.7541636\n",
      "{'后', '分析'}  ||  ADV  ||  0.7225199\n",
      "{'后', '售'}  ||  ATT  ||  0.721961\n",
      "========================\n",
      "标注短语：\n",
      "【工作安排和管理】\n",
      "{'安排', '管理'}  ||  COO  ||  0.91218233\n",
      "{'工作', '管理'}  ||  New!  ||  0.89973116\n",
      "{'安排', '工作'}  ||  COO  ||  0.87946254\n",
      "========================\n",
      "标注短语：\n",
      "【OEM和ODM手机】\n",
      "{'ODM', 'OEM'}  ||  New!  ||  0.96660817\n",
      "{'ODM', '手机'}  ||  ATT  ||  0.9451271\n",
      "{'手机', 'OEM'}  ||  COO  ||  0.93735147\n",
      "========================\n",
      "标注短语：\n",
      "【用例设计方法】\n",
      "{'方法', '用例'}  ||  ATT  ||  0.96015316\n",
      "{'方法', '设计'}  ||  ATT  ||  0.850331\n",
      "========================\n",
      "标注短语：\n",
      "【创建表结构】\n",
      "{'结构', '创建表'}  ||  ATT  ||  0.9999999\n",
      "========================\n",
      "标注短语：\n",
      "【页面元素定位】\n",
      "{'页面', '元素'}  ||  ATT  ||  0.92352164\n",
      "{'元素', '定位'}  ||  SBV  ||  0.8730768\n",
      "========================\n",
      "标注短语：\n",
      "【Web测试】\n",
      "{'Web', '测试'}  ||  ATT  ||  1.0000001\n",
      "========================\n",
      "标注短语：\n",
      "【App测试】\n",
      "{'测试', 'App'}  ||  ATT  ||  1.0000001\n",
      "========================\n",
      "标注短语：\n",
      "【安装与卸载】\n",
      "{'卸载', '安装'}  ||  COO  ||  0.97435725\n",
      "========================\n",
      "标注短语：\n",
      "【App测试】\n",
      "{'测试', 'App'}  ||  ATT  ||  1.0000001\n",
      "========================\n",
      "标注短语：\n",
      "【Web端项目】\n",
      "{'Web', '项目'}  ||  ATT  ||  0.99944466\n",
      "{'端', '项目'}  ||  ATT  ||  0.39551193\n",
      "========================\n",
      "标注短语：\n",
      "【黑盒用例设计方法】\n",
      "{'用例', '黑盒'}  ||  ATT  ||  0.95198977\n",
      "{'方法', '用例'}  ||  ATT  ||  0.87349355\n",
      "{'方法', '设计'}  ||  ATT  ||  0.7704321\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\learningsoft\\anadonda\\lib\\site-packages\\ipykernel_launcher.py:58: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "E:\\learningsoft\\anadonda\\lib\\site-packages\\ipykernel_launcher.py:79: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "E:\\learningsoft\\anadonda\\lib\\site-packages\\ipykernel_launcher.py:46: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "for phrase in phrases[100:200]:\n",
    "    word_pairs,relations,simiscores = getParserResult(phrase)\n",
    "\n",
    "    Z = zip(simiscores,word_pairs,relations)\n",
    "    Z = sorted(Z,reverse=True)\n",
    "    if len(word_pairs)>=1 and len(phrase)>4:\n",
    "        title = \"标注短语：\\n【\"+phrase+'】'\n",
    "        print(title)\n",
    "        for simiscore_,word_pair_,relation_ in Z:\n",
    "    #         Z = zip(word_pair,relation,simiscore)\n",
    "    #         Z = sorted(Z,reverse=True)\n",
    "    #         word_pair_,relation_,simiscore_ = zip(*Z)\n",
    "            print(word_pair_,\" || \",relation_,\" || \",simiscore_)\n",
    "        print(\"========================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentor.release()\n",
    "postagger.release()\n",
    "parser.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

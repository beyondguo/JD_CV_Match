{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\learningsoft\\anadonda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#======================加载word2vec模型==========================\n",
    "from gensim.models import Word2Vec\n",
    "wv = Word2Vec.load(\"F:/Jupyter/--NLP/big_things/models/wikibaikeWV250/wikibaikewv250\")\n",
    "vocab = wv.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\x1c\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.262 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['自然语言', '处理'],\n",
       " ['自然语言', '相关'],\n",
       " ['自然语言', '技术'],\n",
       " ['处理', '相关'],\n",
       " ['处理', '技术'],\n",
       " ['相关', '技术']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================从长短语中获取排列组合的词对===========\n",
    "import jieba\n",
    "def wordsComp(phrase):\n",
    "    words = jieba.lcut(phrase)\n",
    "    comps = []\n",
    "    for i in range(len(words)):\n",
    "        for j in range(i+1,len(words)):\n",
    "            comp = [words[i],words[j]]\n",
    "            comps.append(comp)\n",
    "    return comps\n",
    "\n",
    "phrase = '自然语言处理相关技术'\n",
    "wordsComp(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27191\n",
      "133\n"
     ]
    }
   ],
   "source": [
    "# ==================加载百科词频文件===================\n",
    "import json\n",
    "wiki_word_freq_json = open('F:/Jupyter/--NLP/big_things/wiki_word_freq.json','r',encoding='utf-8').read()\n",
    "wiki_word_freq_dic = json.loads(wiki_word_freq_json)\n",
    "print(wiki_word_freq_dic['能力'])\n",
    "print(wiki_word_freq_dic['java'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\learningsoft\\anadonda\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "#==========================Utility funcitons===========================\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\"\"\"\n",
    "计算一组词的综合词向量，有若干方法：\n",
    "1.简单地直接计算平均，没有的词直接忽略\n",
    "2.同上，但是没有的词，拆成单字来计算词向量\n",
    "3.在上一步的基础上，加上百科词频来赋予权重，然后加权平均\n",
    "4.再加上位置权重\n",
    "\"\"\"\n",
    "# 根据百科词频计算词权重：\n",
    "import math\n",
    "def wiki_weight(word):\n",
    "    try:\n",
    "        freq = wiki_word_freq_dic[word]\n",
    "    except KeyError:\n",
    "        freq = 1\n",
    "    return 1/(math.log10(freq+1))\n",
    "\n",
    "shape_vec = wv[\"加油\"]\n",
    "unk_wv = {}\n",
    "\n",
    "# 计算位置权重：\n",
    "def loc_weight(w_list):\n",
    "    list_len=len(w_list)\n",
    "    i =1-(list_len-1)*0.1\n",
    "    loc_weight_list=[]\n",
    "    for wd in w_list:\n",
    "        loc_weight_list.append(i)\n",
    "        i =i+0.05\n",
    "    return (loc_weight_list)\n",
    "    \n",
    "class Phrase_vec:\n",
    "    # 1.简单地直接计算平均，没有的词直接忽略\n",
    "    def simpleAvgVec(self,words):\n",
    "        vec = np.zeros_like(shape_vec)\n",
    "        n = 0\n",
    "        for word in words:\n",
    "            if word in vocab.keys():\n",
    "                vec += wv[word]\n",
    "                n += 1\n",
    "        if n>0:\n",
    "            return vec/n\n",
    "        else:\n",
    "            return vec\n",
    "    # 2.同上，但是没有的词，拆成单字来计算词向量\n",
    "    def avgVec(self,words):\n",
    "        vec = np.zeros_like(shape_vec)\n",
    "        n = 0\n",
    "        for word in words:\n",
    "            if word in vocab.keys():\n",
    "                vec += wv[word]\n",
    "                n += 1\n",
    "            else:\n",
    "#                 print(word,\" NOT IN WV-DICT ! USE CHAR-VEC\")\n",
    "                chars = [c for c in word]\n",
    "                unk_vec = Phrase_vec.simpleAvgVec(self,chars)*len(word)\n",
    "                vec += unk_vec\n",
    "                n += 1\n",
    "                unk_wv[word] = unk_vec\n",
    "        if n>0:\n",
    "            return vec/n\n",
    "        else:\n",
    "            return vec\n",
    "    \n",
    "    # 3.在上一步的基础上，加上百科词频来赋予权重，然后加权平均\n",
    "    def weightedAvgVec1(self,words):\n",
    "        vec = np.zeros_like(shape_vec)\n",
    "        n = 0\n",
    "        for word in words:\n",
    "            weight = wiki_weight(word)\n",
    "            if word in vocab.keys():\n",
    "                vec += wv[word]*weight\n",
    "                n += 1\n",
    "            else:\n",
    "#                 print(word,\" NOT IN WV-DICT ! USE CHAR-VEC\")\n",
    "                chars = [c for c in word]\n",
    "                unk_vec = Phrase_vec.simpleAvgVec(self,chars)*len(word)\n",
    "                vec += unk_vec*weight\n",
    "                n += 1\n",
    "                unk_wv[word] = unk_vec\n",
    "        if n>0:\n",
    "            return vec/n\n",
    "        else:\n",
    "            return vec\n",
    "    \n",
    "    # 4.再加上位置权重\n",
    "    def weightedAvgVec2(self,words):\n",
    "        loc_weights = loc_weight(words)\n",
    "        vec = np.zeros_like(shape_vec)\n",
    "        n = 0\n",
    "        for word,l_w in zip(words,loc_weights):\n",
    "            weight = wiki_weight(word)\n",
    "            if word in vocab.keys():\n",
    "                vec += wv[word]*weight*l_w\n",
    "                n += 1\n",
    "            else:\n",
    "#                 print(word,\" NOT IN WV-DICT ! USE CHAR-VEC\")\n",
    "                chars = [c for c in word]\n",
    "                unk_vec = Phrase_vec.simpleAvgVec(self,chars)*len(word)\n",
    "                vec += unk_vec*weight*l_w\n",
    "                n += 1\n",
    "                unk_wv[word] = unk_vec\n",
    "        if n>0:\n",
    "            return vec/n\n",
    "        else:\n",
    "            return vec\n",
    "    \n",
    "    \"\"\"\n",
    "    一些短语向量的衡量指标：\n",
    "    ATM: 每个词和整个短语平均词向量的余弦距离\n",
    "    Conicity：锥度，所有ATM的平均值，越大，说明各个词越相近/越聚集\n",
    "    VS（vector spread）：向量散度，衡量各个词的分散性\n",
    "    \"\"\"\n",
    "    def atm(self,word,words):\n",
    "        words_avg_vec = Phrase_vec.avgVec(self,words).reshape(1,250)\n",
    "        current_vec = Phrase_vec.avgVec(self,[word]).reshape(1,250)\n",
    "        atm_score = cosine_similarity(current_vec,words_avg_vec)[0][0]\n",
    "        return atm_score\n",
    "    def conicity_vs(self,words):\n",
    "        atms = [Phrase_vec.atm(self,word,words) for word in words]\n",
    "        sum = 0\n",
    "        for atm in atms:\n",
    "            sum += atm\n",
    "        conicity_score = sum/len(words)\n",
    "        minus_square = 0\n",
    "        for atm in atms:\n",
    "            minus_square += pow((atm-conicity_score),2)\n",
    "        vs_score = minus_square/len(words)\n",
    "        return [10*conicity_score,1000*vs_score]\n",
    "        \n",
    "\n",
    "p_vec = Phrase_vec()\n",
    "\n",
    "def getSimiScore(words,target_words):\n",
    "    words_vec = p_vec.weightedAvgVec1(words).reshape(1,250)\n",
    "    target_vec = p_vec.weightedAvgVec1(target_words).reshape(1,250)\n",
    "    return cosine_similarity(words_vec,target_vec)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.799393385648727   7.039833068847656   6.8465059995651245\n",
      "0.05617520905221163   0.5096023786546056   7.38286402438959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\learningsoft\\anadonda\\lib\\site-packages\\ipykernel_launcher.py:53: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "conicity,vs两个指标的试验：\n",
    "\"\"\"\n",
    "conicity,vs = p_vec.conicity_vs(['开心','高兴','兴奋','心情'])\n",
    "conicity1,vs1 = p_vec.conicity_vs(['信息','管理','系统','开发'])\n",
    "conicity2,vs2 = p_vec.conicity_vs(['昨天','电脑','物理','书籍'])\n",
    "print(conicity,' ',conicity1,' ',conicity2)\n",
    "print(vs,' ',vs1,' ',vs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#=======================Sum up, get final results==================\n",
    "\"\"\"\n",
    "input: \n",
    "    sentence/短语\n",
    "output:\n",
    "    word_pairs---list\n",
    "    relations---list\n",
    "    simiscores---list\n",
    "\"\"\"\n",
    "def getParserResult(sentence):\n",
    "    target_words = jieba.lcut(sentence)\n",
    "    word_comps = wordsComp(sentence)\n",
    "    simiscores = []\n",
    "    for word_comp in word_comps:\n",
    "        score = getSimiScore(word_comp,target_words)\n",
    "        simiscores.append(score)\n",
    "    return [word_comps,simiscores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "ann_filenames = []\n",
    "phrases = []\n",
    "kind = 'Reserved_Employee'\n",
    "base_path = 'F:/Jupyter/--NLP/big_things/resume_txt_ann_6kinds'\n",
    "files = os.listdir(base_path+\"/\"+kind)\n",
    "ann_filenames = [f for f in files if os.path.splitext(f)[-1]=='.ann']\n",
    "for index,ann_filename in enumerate(ann_filenames):\n",
    "    with open(base_path+\"/\"+kind+\"/\"+ann_filename,encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            phrase = l.split('\\t')[-1].strip()\n",
    "#             phrase = re.sub('[a-zA-Z]','',phrase)\n",
    "            if len(jieba.lcut(l))>=2:\n",
    "#             if len(phrase)>4:\n",
    "                phrases.append(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标注短语：\n",
      "【客户满意度】\n",
      "['客户', '满意度']  ||  1.0\n",
      "========================\n",
      "标注短语：\n",
      "【钢结构检测】\n",
      "['钢结构', '检测']  ||  1.0\n",
      "========================\n",
      "标注短语：\n",
      "【钢筋保护层检测】\n",
      "['钢筋', '保护层']  ||  0.9610065\n",
      "['钢筋', '检测']  ||  0.94737816\n",
      "['保护层', '检测']  ||  0.93412316\n",
      "========================\n",
      "标注短语：\n",
      "【结构荷载试验】\n",
      "['荷载', '试验']  ||  0.96668375\n",
      "['结构', '荷载']  ||  0.9378102\n",
      "['结构', '试验']  ||  0.8408343\n",
      "========================\n",
      "标注短语：\n",
      "【工程质量鉴定】\n",
      "['工程质量', '鉴定']  ||  1.0000001\n",
      "========================\n",
      "标注短语：\n",
      "【建筑物沉降观测】\n",
      "['沉降', '观测']  ||  0.9560437\n",
      "['建筑物', '沉降']  ||  0.9416058\n",
      "['建筑物', '观测']  ||  0.88247716\n",
      "========================\n",
      "标注短语：\n",
      "【房屋抗震鉴定】\n",
      "['抗震', '鉴定']  ||  0.94098055\n",
      "['房屋', '抗震']  ||  0.9289453\n",
      "['房屋', '鉴定']  ||  0.86611235\n",
      "========================\n",
      "标注短语：\n",
      "【数据分析整理】\n",
      "['数据分析', '整理']  ||  0.99999994\n",
      "========================\n",
      "标注短语：\n",
      "【气化器的设计】\n",
      "['气化', '器']  ||  0.9394505\n",
      "['气化', '设计']  ||  0.8978141\n",
      "['气化', '的']  ||  0.864365\n",
      "['器', '设计']  ||  0.79427195\n",
      "['器', '的']  ||  0.74281216\n",
      "['的', '设计']  ||  0.6039327\n",
      "========================\n",
      "标注短语：\n",
      "【挖掘客户潜在需求】\n",
      "['潜在', '需求']  ||  0.8872643\n",
      "['客户', '潜在']  ||  0.8820095\n",
      "['挖掘', '客户']  ||  0.86234796\n",
      "['客户', '需求']  ||  0.86049145\n",
      "['挖掘', '潜在']  ||  0.85461366\n",
      "['挖掘', '需求']  ||  0.85448444\n",
      "========================\n",
      "标注短语：\n",
      "【维护和跟进现有买家资源】\n",
      "['维护', '买家']  ||  0.859174\n",
      "['跟进', '资源']  ||  0.8589796\n",
      "['跟进', '买家']  ||  0.8310205\n",
      "['跟进', '现有']  ||  0.83076257\n",
      "['维护', '跟进']  ||  0.82414615\n",
      "['现有', '买家']  ||  0.8146506\n",
      "['买家', '资源']  ||  0.8137298\n",
      "['维护', '资源']  ||  0.77696514\n",
      "['现有', '资源']  ||  0.7739458\n",
      "['维护', '现有']  ||  0.76011217\n",
      "['和', '跟进']  ||  0.7583173\n",
      "['和', '买家']  ||  0.7383764\n",
      "['和', '资源']  ||  0.73435664\n",
      "['维护', '和']  ||  0.70084995\n",
      "['和', '现有']  ||  0.67973244\n",
      "========================\n",
      "标注短语：\n",
      "【产品检验标准】\n",
      "['产品检验', '标准']  ||  1.0000002\n",
      "========================\n",
      "标注短语：\n",
      "【新产品评审】\n",
      "['产品', '评审']  ||  0.92753196\n",
      "['新', '评审']  ||  0.91433257\n",
      "['新', '产品']  ||  0.7912396\n",
      "========================\n",
      "标注短语：\n",
      "【制订生产计划】\n",
      "['制订', '生产']  ||  0.9470096\n",
      "['制订', '计划']  ||  0.92082775\n",
      "['生产', '计划']  ||  0.8770149\n",
      "========================\n",
      "标注短语：\n",
      "【生产现场管理制度】\n",
      "['现场', '管理制度']  ||  0.94244915\n",
      "['生产', '管理制度']  ||  0.92633724\n",
      "['生产', '现场']  ||  0.7938124\n",
      "========================\n",
      "标注短语：\n",
      "【维护客户关系】\n",
      "['维护', '客户关系']  ||  1.0000001\n",
      "========================\n",
      "标注短语：\n",
      "【生产部考核标准】\n",
      "['生产', '考核']  ||  0.89259374\n",
      "['部', '考核']  ||  0.89050853\n",
      "['考核', '标准']  ||  0.85636866\n",
      "['部', '标准']  ||  0.8010027\n",
      "['生产', '标准']  ||  0.7522389\n",
      "['生产', '部']  ||  0.74408066\n",
      "========================\n",
      "标注短语：\n",
      "【生产线例会】\n",
      "['生产线', '例会']  ||  0.99999994\n",
      "========================\n",
      "标注短语：\n",
      "【协调线体生产】\n",
      "['协调', '线体']  ||  0.94214034\n",
      "['线体', '生产']  ||  0.91742146\n",
      "['协调', '生产']  ||  0.81728745\n",
      "========================\n",
      "标注短语：\n",
      "【利用率测算】\n",
      "['利用率', '测算']  ||  0.9999999\n",
      "========================\n",
      "标注短语：\n",
      "【原代细胞培养】\n",
      "['原代', '细胞培养']  ||  1.0\n",
      "========================\n",
      "标注短语：\n",
      "【细胞系培养】\n",
      "['细胞系', '培养']  ||  1.0\n",
      "========================\n",
      "标注短语：\n",
      "【丝网印刷机造作】\n",
      "['印刷机', '造作']  ||  0.92568517\n",
      "['丝网', '造作']  ||  0.91497195\n",
      "['丝网', '印刷机']  ||  0.90270674\n",
      "========================\n",
      "标注短语：\n",
      "【精装书查书堵头】\n",
      "['精装书', '查书']  ||  0.9983734\n",
      "['查书', '堵头']  ||  0.99740225\n",
      "['精装书', '堵头']  ||  0.55774325\n",
      "========================\n",
      "标注短语：\n",
      "【布刷胶衬页封皮】\n",
      "['布刷胶', '衬页']  ||  0.9994061\n",
      "['布刷胶', '封皮']  ||  0.9603859\n",
      "['衬页', '封皮']  ||  0.7527305\n",
      "========================\n",
      "标注短语：\n",
      "【语音文件转文字】\n",
      "['语音', '文件']  ||  0.889477\n",
      "['语音', '转']  ||  0.8770631\n",
      "['语音', '文字']  ||  0.86371017\n",
      "['转', '文字']  ||  0.85825133\n",
      "['文件', '文字']  ||  0.847196\n",
      "['文件', '转']  ||  0.80527985\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\learningsoft\\anadonda\\lib\\site-packages\\ipykernel_launcher.py:74: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "E:\\learningsoft\\anadonda\\lib\\site-packages\\ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "for phrase in phrases[100:200]:\n",
    "    word_comps,simiscores = getParserResult(phrase)\n",
    "\n",
    "    Z = zip(simiscores,word_comps)\n",
    "    Z = sorted(Z,reverse=True)\n",
    "    if len(word_comps)>=1 and len(phrase)>4:\n",
    "        title = \"标注短语：\\n【\"+phrase+'】'\n",
    "        print(title)\n",
    "        for simiscore_,word_comps_ in Z:\n",
    "            print(word_comps_,\" || \",simiscore_)\n",
    "        print(\"========================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['黑盒', '用例', '设计']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.lcut(\"黑盒用例设计\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
